{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNeZuWUovWt8"
   },
   "source": [
    "### Project Decription\n",
    "\n",
    "<font color='blue'>In this hands-on project the goal is to build a python code for image classification from scratch to understand the nitty gritties of building and training a model and further to understand the advantages of neural networks. First we will implement a simple KNN classifier and later implement a Neural Network to classify the images in the SVHN dataset. We will compare the computational efficiency and accuracy between the traditional methods and neural networks.</font>\n",
    "\n",
    "### Project Objectives\n",
    "\n",
    "<font color='blue'>The objective of the project is to learn how to implement a simple image classification\n",
    "pipeline based on the k-Nearest Neighbour and a deep neural network. The goals of this\n",
    "assignment are as follows:\n",
    "<br><br>● Understand the basic Image Classification pipeline and the data-driven\n",
    "approach (train/predict stages)\n",
    "<br>● Data fetching and understand the train/val/test splits.\n",
    "<br>● Implement and apply an optimal k-Nearest Neighbor (kNN) classifier (7.5\n",
    "points)\n",
    "<br>● Print the classification metric report (2.5 points)\n",
    "<br>● Implement and apply a deep neural network classifier including (feedforward\n",
    "neural network, RELU activations) (5 points)\n",
    "<br>● Understand and be able to implement (vectorized) backpropagation (cost\n",
    "stochastic gradient descent, cross entropy loss, cost functions) (2.5 points)\n",
    "<br>● Implement batch normalization for training the neural network (2.5 points)\n",
    "<br>● Understand the differences and trade-offs between traditional and NN\n",
    "classifiers with the help of classification metrics (5 points)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "colab_type": "code",
    "id": "Ib0wdL-CuaQN",
    "outputId": "a279963e-802e-436e-a423-6aeb33f1ff56"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Importing important modules\n",
    "#!pip install sklearn\n",
    "import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization,Activation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from tensorflow.keras import backend\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "rSUkPbIZnIin",
    "outputId": "9e787c62-fed7-4a0f-a32d-fe8d88e8c47e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VnHBJQQH_cCv"
   },
   "source": [
    "## Data fetching and understand the train/val/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IuvRMJFFqyOH"
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Open the file as readonly\n",
    "h5f = h5py.File('/content/drive/My Drive/GL AIML/SVHN_single_grey1.h5', 'r')\n",
    "\n",
    "# Load the training, test and validation set\n",
    "x_train = h5f['X_train'][:]\n",
    "y_train = h5f['y_train'][:]\n",
    "x_test = h5f['X_test'][:]\n",
    "y_test = h5f['y_test'][:]\n",
    "\n",
    "\n",
    "# Close this file\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "ip50CCzgKVGt",
    "outputId": "5cb5da59-8de8-49fb-f515-68b05cd9fa07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data points: 42000\n",
      "testing data points: 18000\n"
     ]
    }
   ],
   "source": [
    "print(\"training data points: {}\".format(len(y_train)))\n",
    "print(\"testing data points: {}\".format(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DTuluB_k9U1H",
    "outputId": "ce44c8ae-af70-4c5c-dc94-45e9866bfdb6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "RSaMjeu1-NW-",
    "outputId": "1123b97e-46f0-4b19-be0e-62ac4932d210"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc6f2196438>"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAea0lEQVR4nO2de5Bd1XXmv3VfffulfurReiEJSQgh\nkAAZw4A9DE4cTJzCrskQu1IOqTiRMxVXjWs8NcUwVWOnav6wp2Ic/phySh4zJh7HmNi4TDKUE5ny\nBOMQgXBAEgj0srDU6C21utXP+1jzx72qCGp/u1v9uC2zv1+VSt179T5nnX3Puqd7f3etZe4OIcR7\nn8x8OyCEaAwKdiESQcEuRCIo2IVIBAW7EImgYBciEXIzmWxm9wJ4FEAWwP9y9y/Ffr6Qb/VioTN8\nrJgEWK1Oxzlq8uz03uOsSnxk40Dc99g1c/ej14ZsNnyqHL/mmPiaKUf8r1QiE8n5KpHjxdYqE7nm\n2HqwhYzMqRbCawgAlabYPO6F80PSFyBT4lOy4+FJ4yPnURofDjo57WA3syyA/wng1wEcA/CSmT3t\n7q+zOcVCJ27f9Jnw8cb5ldnoRHg8cuN4U57aKu1FaovdVJmxctiPkXE6x0bGqM3Hw9cFABZ7Q8rz\na6v2LAiOl7r4NXvkmgtnR6ktc3aQH7MlfD67OMLnDA9TmzU3UxsKfD1YUHtkzviK8AMJAC6s4RF9\ncTlfx4lOfq+yoG7t5/dAx+HwvfjK/3uUn4daJuc2AAfd/bC7TwB4AsD9MzieEGIOmUmwLwNw9LLv\nj9XHhBBXIXO+QWdm28xsl5ntKpX5r2lCiLllJsHeD2DFZd8vr4+9A3ff7u5b3X1rPtc6g9MJIWbC\nTIL9JQDrzGy1mRUAfALA07PjlhBitpn2bry7l83sswD+DjXp7TF3fy02x8oVZE9fIMaIpNEe3okd\nWd5O5wyu5JdWWhDZcecb5Gg5Fd5R7TjA/zzJjkZ26otN1Fbu66K2keUt1HZ+XVjjGd7A/Si28Yse\nG+TnaurvoLaufWFpqHPPAJ1j5fAO86TEZNZceD1GV/P1PXE733Fve98Zavu9la9S29LCeWqrePiZ\n+/LFVXTOjgMbguOlV7mQOiOd3d2fAfDMTI4hhGgM+gSdEImgYBciERTsQiSCgl2IRFCwC5EIM9qN\nv2IMPBtqhCdcjG5YFBzv/yB3v+em09S2qesUtZ0b5x/82ffP1wTHmwZ4kknrMS4nVZb2UNup2yKy\n4vv5Wv3m9XuD41vbfkHndGZ5ckrBeGbboYnw6wIAf/7Kh4Lj4x1c8lq0i7+e2WP89USJr3G1N5wY\ndPpmLq9t/vAb1PYHi3/KzxV5dr4+xj9J3p27GBzf1vsPdM7dHfuC4w+3cmlTT3YhEkHBLkQiKNiF\nSAQFuxCJoGAXIhEauhvv2Qwq3W1BWyZShmm4L+xm03peFumevv3Utr75BLXtGlpNbW+UVwXHs+OR\n2mmR8keD6/iO+9AdfMf99zbtpLbF+XCi0VMnb6Vzzo3xZJd7+2iVMdzV9ia1/fltTwTHH275eMQP\nXg5q4Rn+Wsc4fWt4jTvu5vfAw8t4ukeJJK0AwEOH/y21Hf3Zcmqb6A7fP5+48wU653e7wvdAs/Hy\nbnqyC5EICnYhEkHBLkQiKNiFSAQFuxCJoGAXIhEanAhjtLVOaSVPkBgkatjqTv6h/74CqXUH4O0J\nfq4f/2I9tfW+Eq7vVdx/ks6pdoSlRgA4v4G/1354fTjRAeDyGgA8/tbtwfHS93jSSssZnuzy9fs/\nQG3X3Mnrsf1Gyy+D43+47h/pnEdvuI/auveGE1oARGvQnd8UlrU+v/IlOqfVeGLNS+MrqO3gwSXU\ntvp5LokNLQvLsztWhevMAcADnWH/o628IjYhxHsIBbsQiaBgFyIRFOxCJIKCXYhEULALkQgzkt7M\n7AiAIQAVAGV33xr7ec8YKsXwKc9s4q2QWm46Fxy/q/cQnbMwx7OkLlTC7aQAYPwCryfXcShcq606\nwKWw0Rv7qK16fbj2GAD8fi+vdVYAz7L7P/b+4HhmkM9pfzEskwFA2/o11LbvlqXUdl/L0eD4R9t4\nh7Bvbgj7DgBDa7qpLVPiglP7ivB9cE8Lz9hr4UoeTpe5BJgb4OHUfOBtarNKb3D8yACvh3i2Gs5U\nLEey8mZDZ/837s4FVyHEVYF+jRciEWYa7A7g783sZTPbNhsOCSHmhpn+Gn+Xu/eb2SIAO8zsDXd/\n7vIfqL8JbAOApiKvRCKEmFtm9GR39/76/6cA/ADAbYGf2e7uW919az7PNxyEEHPLtIPdzFrNrP3S\n1wA+DCDcjkQIMe/M5Nf4xQB+YLWMoxyAv3L3H0VnZIBKMfz+MnQtz7z6dyvD7XjWF4/TOWNVXuix\nJTNBbdbE/bBS2JZp5QUbh1bwJb52UT+1tUcKB2aNS0039YQlnme3LKZzWk5xeTB/kZ9r9wXe0ugE\n+Yvt+gLPArx76UFq+9G14Ww+AMjx7lVY3RWWbXuy/Loy4NpbS2ac2qr5WM4ZJzsSzrLLRY43VA3L\nx7EWVNMOdnc/DGDzdOcLIRqLpDchEkHBLkQiKNiFSAQFuxCJoGAXIhEa2+stYyi1hQtOejOXvEYr\nYRntZIl/Iq8jO0xt3TmebZaJSDLVQni5Mgu4nDTRwWWc9QtOUdu4h9cJADKRsoI3tIblvB8tu4HO\nmWjnMmVhiJ9rYIxnDw55+JgV59l3N7Yeo7YfdnM/PMvXuC0flspajK/vyQovOFmKvC7VVn4PezPP\n6mT+m/G1YtJyNSIb6skuRCIo2IVIBAW7EImgYBciERTsQiRCY9s/AbBKeFe1cIK78pOj64Lj48v4\nnA91vk5t2chudrXE3/8yEySB5jyvd5cb4QkoVec7p8tzfEe4GNlJPpAL18OzLN/ZzQ/z9agU+U59\njIUkYSRrXLkYqPCEogxfDmR5bgoGS+GagqWIKlCJ7GifKbfzk0XwfERdmQjv4pcm+P3dnh0Njmcj\n9Qn1ZBciERTsQiSCgl2IRFCwC5EICnYhEkHBLkQiNFR6y5SqaDkR1kl6d/NEgdO5juB4fxdPhOnp\n4ckurH7XZHg2/N5YPX+ezunex3Whv9lzE7Wta+ZJMtc18VZCLw6H2zVl+nlbq9wAlw59YYHamnO8\nTl5rJtJDiXCmxGWt/CA/XuvbXG46dKYnOD68ms9ZnuXXHK1fOMGfnZ6bhi1S0q49MxYcz0TqE+rJ\nLkQiKNiFSAQFuxCJoGAXIhEU7EIkgoJdiESYVHozs8cAfBTAKXffVB/rBvBdAKsAHAHwgLtz/enS\nsUoV5I8PBG0dA1x6G1oZlk9iWWMxYq2hsgVeR2yiOyxfFXN8GZteOkBtfQs3Utsj+DVqa27nct74\n0XBW2bIX+HVlT5yltspGLoeNlvk6HibZZicqYckIAHaeXUVtxbNcUup8k8usF14Ny7ZPXsdlz22R\njMnNzW9RW35ROBMNAIZX8my/cnP4Ps7m+PGGquH1jcXEVJ7s3wRw77vGHgLwrLuvA/Bs/XshxFXM\npMFe77f+7u549wN4vP714wA+Nst+CSFmmen+zb7Y3S+1UD2BWkdXIcRVzIw36NzdEflgn5ltM7Nd\nZrZrohLprSuEmFOmG+wnzawPAOr/0w9yu/t2d9/q7lsLWV52SAgxt0w32J8G8GD96wcB/HB23BFC\nzBVTkd6+A+BuAL1mdgzAFwB8CcCTZvZpAG8BeGBKZ3MHSqRyIMkoAwAjqlEhw+Wk4SqX8iqR97hc\nnh+zUgxnQ1kx0tqHXS+ArhdPUNuCQ1zyYtl3AGDjQ8HxzBGeKVctR9odtXIppz1SFJNxtMwzFQ8f\n76W25W9HpMPj794//he69oclr/994HY655abjlDbKlLQEwBuXX6U2nZ+4DpqY22jPrL2TTpnRS4s\nYedZsGAKwe7unySmD002Vwhx9aBP0AmRCAp2IRJBwS5EIijYhUgEBbsQidDYXm9mQJ6cMsd7YTl5\nSypGCh5WI+9jJefnmhZNXHozi7yfeqQ44OF+fsx8pP+ahaWymLxmS/mnnUcXc+ltZStPdCxa+Hyv\njy2jc7JHeVHMlqNhqQkAPHJtbUfDWXbDz3EJ8E/bfovavrDmb6jtPy/9EbUd+a2XqG2wEr7uO5uP\n0DlDkcxNhp7sQiSCgl2IRFCwC5EICnYhEkHBLkQiKNiFSITGSm+RrDeLyFCMnPF+XUXjstx0qeTD\nMpRlYn28Iks8wX20Zt6PzpsjUh85ZqaV1xKYWBIpKrmc+3hDG8+kO1UJZ5t96+BtdE7PnogUOcSL\nLyKSdZg7Fe5j136Uz3mrn2ffvbaMS4d/3MGLUZ6tcv/3ji4Pjm9oOh4cB4Aq0aOrmFnBSSHEewAF\nuxCJoGAXIhEU7EIkgoJdiERo8G48gCrbQefJKWzTvcwyZADkSSIGAGTAd/GbmyaozTOt4fFSZFc9\nkiTjkd34KKQWXpQmPufCap6A0rmEJ7u0ZHgbqr94++7geObZLjqn61XehgqjvG1U7NqsEn6ty0W+\na93czs8VS6J69PxaavvWIa5CDJwL31enbuQqyYM9PwuOxxqi6ckuRCIo2IVIBAW7EImgYBciERTs\nQiSCgl2IRJhK+6fHAHwUwCl331Qf+yKAPwJwuv5jD7v7M5OeLVaDjtROA4AMUcPGyldehwsAWtkB\nATQXInIYy9OISWiRhBZUeaueaCJMZK0wHr62ieuW0ilnt/AElPf3nqS2l4dWUdueF8Iy1Nqf8lpy\n6OftsNDFa8Y5u6cAjKztDo6f/ABf+/+y8SfUtigXTqwBgC8duJfaMs9wyXHVkfD987eDt9I5//oj\n4dZQE36G+0At/8I3AYSu4qvuvqX+b/JAF0LMK5MGu7s/B4B3zhNC/Eowk7/ZP2tmu83sMTPjv6MI\nIa4KphvsXwNwLYAtAI4D+Ar7QTPbZma7zGzXRGVkmqcTQsyUaQW7u59094q7VwF8HQD94K+7b3f3\nre6+tZDl1VKEEHPLtILdzPou+/bjAPbOjjtCiLliKtLbdwDcDaDXzI4B+AKAu81sC2pi1BEAn5nS\n2czghSuXy6waloZi9bbyxqWV1ki2ViHL5400xXKKCLGMuFgbpyx/H7ZR7r+3hX97OrOJS3l9G7jk\n1Zzl/v947/XUtvL58DracS4NIRtpy1Xmr4u382s7e0N4je/ZvJvO+Z32Q9S2dyJS7y7DsyltiMub\nLbuPBcdbb1xD5+weXREcH61y3ycNdnf/ZGD4G5PNE0JcXegTdEIkgoJdiERQsAuRCAp2IRJBwS5E\nIjS24KSBZ7dF2j8ZMZWr/L1qpMolklihxEKGSzwXmVKW4ZKcx66rECkcGZGaYoysD7cuGriZZ/rd\n0cPbOP3T8WuorfdnXDpsPRxOp4jKjbFsvjIvIFpewAtmXlwTnvc7vTvpnI4Ml/LGIgUnY9JbJdbd\njBTMrEaW6sT4guB4rCCmnuxCJIKCXYhEULALkQgKdiESQcEuRCIo2IVIhAb3enNYiUsofN6VT4n1\neiuAy1r5SNZbla1Wji+jZSLvp5HMtli2XGXFImo7+b6wXrNmVTizCgAODC6ktvEXeqht5cu8D5xd\nuBgc90ifOqtEbsdIr7eJLn7MtiVhP5bkhuicYxGZb6DCC3eOlLhWVhzlshyTHCMqGsrVsNE9UriV\nH04I8V5CwS5EIijYhUgEBbsQiaBgFyIRGrwbj2jCC8PIRmYlkghTcW6bsEiyAMu6AcAOGa8lN726\najFGlvFEjerG8O7zslbedumnr2ygtut28HZHdoy3hkIxnJxilciudDVii1Ap8B3o3rbh4Hge/Fzj\nkVu0Eql7GNsJr+ZiST7h+yDL87VwoRRe39h9rye7EImgYBciERTsQiSCgl2IRFCwC5EICnYhEmEq\n7Z9WAPhLAItRE8+2u/ujZtYN4LsAVqHWAuoBd+eZEUBtOpFXvDUiJxEvR8tc8jpdCdfoAoBW55rG\nRCUiyxGlzMciGkmk3ZVfDMtCAGDLllDb6c38Zdu8rD84vv88T57pfTFyzft/SW3Wwl8z5EiiRmQ9\nSkva+eHOj3I/IlLZwEjER0J3JHlpQYYn5CwoctvFZn5MbycNTyNq3dAEk95mlghTBvB5d98I4HYA\nf2JmGwE8BOBZd18H4Nn690KIq5RJg93dj7v7z+tfDwHYB2AZgPsBPF7/sccBfGyunBRCzJwr+pvd\nzFYBuBnATgCL3f143XQCtV/zhRBXKVMOdjNrA/B9AJ9z93d8htJrxdGDfzmZ2TYz22VmuyYq/O8u\nIcTcMqVgN7M8aoH+bXd/qj580sz66vY+AKdCc919u7tvdfetheyVb5YIIWaHSYPdzAy1fuz73P2R\ny0xPA3iw/vWDAH44++4JIWaLqWS93QngUwD2mNkr9bGHAXwJwJNm9mkAbwF4YNIjOaaV6cWy3qoR\nmaFovIZbdzacGQYAnU38T43jrax1Fc+gsiJvQ2WR2nUXbgy3cQIAu5FnorE1ObOPH2/NGyPUFiWS\n7Te2Niz1nbqVr8fFdfw1azncTW2tb3PtbXAo/NvkyUobndOe4esby3obGOVtqAqR/k+eI8/cWIJg\nTJcjTBrs7v48uOL3oSs+oxBiXtAn6IRIBAW7EImgYBciERTsQiSCgl2IRGhswUmAtrqxIS7/FAfC\nssvwOG/7U8xMUFus/VNMzqNtqCyS0RTLiOvuoKZz1/NMtPULT1PbsaHO4HjbEe5j7iyXIq2DZw/6\nglZqG+sJy3IXr+Ovy3Wrj1Pb/rHl1Nazj8tadiIs9e2f4FmFiyPS7HCVS4fNed42aqwlcl+xLLvI\nozgzjZ5oerILkQgKdiESQcEuRCIo2IVIBAW7EImgYBciERorvRmALMvw4VJCphy2lSO93rIRaSLD\n0ugAjJS5nMcKTiIT6+PF5ZiJSIHF0dVcotrU8TY/nYclqjeX8/Xo/01eZGiCu4hqEz9mhSzjHRsO\n0Tkf7NpPbV9+Yxm1tfTzTMXWY+EL2DvMj3dvK/djfT5YtgEAsKhliNqO5Pka28Ww/9nRsIwKAM25\ncIZgrFehnuxCJIKCXYhEULALkQgKdiESQcEuRCI0OBHGaCKMt/AEg1JzeE4uEynSNU3a8jxxpUTK\nllkzr5rrJV5X7eJSvvPfvegcta0v8oSRtUtOBscP/sYx7keFr/2yJt7Ra7zKa9DlLSxdfLR9N50z\n5jz5J1ZyLTPMX7PufeFr+79vbKJzNrZwtWNLkbfD6mniyVxvdPMLKC0JJ0Sx+w0AFjeH6+TlqWSk\nJ7sQyaBgFyIRFOxCJIKCXYhEULALkQgKdiESYVLpzcxWAPhL1FoyO4Dt7v6omX0RwB8BuFQQ7WF3\nfyZ6sGoVPjoWPk+JJ4ywz/bHEmEGq7wVTybSV6eQ4X5U2WoVuASFsfD1AsDoQu7/NQt4C6ICkbUA\nYGEuLJVtaTpK58SSJ2IJRUNVLh2OeXhNYrXT+ss88QNN/DXzAr+NW94MJ650PM8TYR7r+lfU9h/X\ncnntfQt+QW177uyjtoNrwte9ZGlYRgWAOxaEE4p2ZLgMORWdvQzg8+7+czNrB/Cyme2o277q7n82\nhWMIIeaZqfR6Ow7geP3rITPbB4C/LQohrkqu6G92M1sF4GYAO+tDnzWz3Wb2mJl1zbJvQohZZMrB\nbmZtAL4P4HPuPgjgawCuBbAFtSf/V8i8bWa2y8x2TVR5kQEhxNwypWA3szxqgf5td38KANz9pLtX\n3L0K4OsAbgvNdfft7r7V3bcWMvwz5EKIuWXSYDczA/ANAPvc/ZHLxi/fXvw4gL2z754QYraYym78\nnQA+BWCPmb1SH3sYwCfNbAtqctwRAJ+Z9EhmsHxYkql2Ruqx9YTfk5Ys4DW/YpTAs6s2tHK542fX\nXRscH9zM64u1HuEtkkaXcBnq1k6eXbU0zzPRinbltcli2WZDEQkz1gqpMxuWqA6Xw628AOBERHrL\nNnG5cWxxC7UVD4Ulx0U7+f12rGMhtT2CX6O2B1f/E7X9wep/pLbzK8L3SFdumM55bSS8Rz5W3Ufn\nTGU3/nmEEwzjmroQ4qpCn6ATIhEU7EIkgoJdiERQsAuRCAp2IRKhsQUnc1lUe8PF9c7dGB4HgAvX\nhzPRfnvhATqnNZL9U3H+Hre2eILarl8Rth2+YTWdA+NVA8stPJOrFJHDTpcXUNtAJSxDxSS0WGbb\nmTL3nxWVBICObPjTkufKXIociWTRVUb5elSK/PU0UuDUfsmLdq74O74eF470UNuf3XMvtS295iy1\nZYksOjTGpc2hg2GZ8vzQC3SOnuxCJIKCXYhEULALkQgKdiESQcEuRCIo2IVIhIZKb26AEykk0jaM\n8vOBFdT2D6fXUdvQOJc0Opp4gchDJ8LZUK2R5Lumc7zXW9dr3I9vN99ObU91baa2sYvhY3ol0iyt\nxN/zcxci/dcisCQ7kpQ3KX17uRzWvpvLaGyWFbjMZ6cHqK2DZNEBQNeLPKNvdB3PpKvmw69NV4lf\n86LBcFbhmfNcztWTXYhEULALkQgKdiESQcEuRCIo2IVIBAW7EInQUOnNqo7M2ETQ1nmAS15Ng2GZ\n5PSONXROYZD3bGuJqVDg2XdrRsLHzA7zApA2Gr5eAFh8istai16Y3vuwlcn5nMs4Nh7Rwya4zcf5\ntaEcXitr4eXEvZlLkVbhklLMR2snWXu5iKQYOR6qkZ5zA7w/X/NrkWNmyGudjdwDZI6VeSainuxC\nJIKCXYhEULALkQgKdiESQcEuRCJMuhtvZkUAzwFoqv/899z9C2a2GsATAHoAvAzgU+4e2Z4Fqvks\nxpeG66c1vXWOziscJvXkSFINMMlO8TivT4d8ZElyxNbO66rhwkVq8iGeQVON+RjbWSc+RhM/OnhN\nO2/lu+cWWSu/GE7UQIXvFtto5Joju+Cx9UCW7LpH7h0UuSqQYfcAEPcxtlZN5LWJKAaeJf6znX1M\n7ck+DuAed9+MWnvme83sdgBfBvBVd18L4DyAT0/hWEKIeWLSYPcalx5P+fo/B3APgO/Vxx8H8LE5\n8VAIMStMtT97tt7B9RSAHQAOARhw90ufnDgGINxWUghxVTClYHf3irtvAbAcwG0ANkz1BGa2zcx2\nmdmuUom3oBVCzC1XtBvv7gMAfgLgDgCdZnZp12E5gH4yZ7u7b3X3rfl8ZCNLCDGnTBrsZrbQzDrr\nXzcD+HUA+1AL+t+u/9iDAH44V04KIWbOVBJh+gA8bmZZ1N4cnnT3vzWz1wE8YWb/HcA/A/jGZAeq\nFgxDy8MyQ/EXEfmE1a3rbudTSlziQSSpwmJJEOxwnfw3lgyTVQBYF5e8sjFpKAZLnogcr8pkHADV\nFu4/qhEJsCf82lSb+C1XKUakpshyZCrcD8+EJ1aa+HOu3MJt7HhALdGLUSF15gBeg64aydVxsoyl\nfj5p0mB3990Abg6MH0bt73chxK8A+gSdEImgYBciERTsQiSCgl2IRFCwC5EI5rGModk+mdlpAG/V\nv+0FcKZhJ+fIj3ciP97Jr5of17h7sNdUQ4P9HSc22+XuW+fl5PJDfiToh36NFyIRFOxCJMJ8Bvv2\neTz35ciPdyI/3sl7xo95+5tdCNFY9Gu8EIkwL8FuZvea2ZtmdtDMHpoPH+p+HDGzPWb2ipntauB5\nHzOzU2a297KxbjPbYWYH6v93zZMfXzSz/vqavGJm9zXAjxVm9hMze93MXjOz/1Afb+iaRPxo6JqY\nWdHMXjSzV+t+/Gl9fLWZ7azHzXfNLJKSGMDdG/oPQBa1slZrABQAvApgY6P9qPtyBEDvPJz3gwBu\nAbD3srH/AeCh+tcPAfjyPPnxRQD/qcHr0QfglvrX7QD2A9jY6DWJ+NHQNQFgANrqX+cB7ARwO4An\nAXyiPv4XAP79lRx3Pp7stwE46O6HvVZ6+gkA98+DH/OGuz8H4N21s+9HrXAn0KACnsSPhuPux939\n5/Wvh1ArjrIMDV6TiB8NxWvMepHX+Qj2ZQCOXvb9fBardAB/b2Yvm9m2efLhEovd/Xj96xMAFs+j\nL581s931X/Pn/M+JyzGzVajVT9iJeVyTd/kBNHhN5qLIa+obdHe5+y0APgLgT8zsg/PtEFB7Z0ft\njWg++BqAa1HrEXAcwFcadWIzawPwfQCfc/d39D9u5JoE/Gj4mvgMirwy5iPY+wGsuOx7WqxyrnH3\n/vr/pwD8APNbeeekmfUBQP3/U/PhhLufrN9oVQBfR4PWxMzyqAXYt939qfpww9ck5Md8rUn93Fdc\n5JUxH8H+EoB19Z3FAoBPAHi60U6YWauZtV/6GsCHAeyNz5pTnkatcCcwjwU8LwVXnY+jAWtiZoZa\nDcN97v7IZaaGrgnzo9FrMmdFXhu1w/iu3cb7UNvpPATgv86TD2tQUwJeBfBaI/0A8B3Ufh0sofa3\n16dR65n3LIADAH4MoHue/PgWgD0AdqMWbH0N8OMu1H5F3w3glfq/+xq9JhE/GromAG5CrYjrbtTe\nWP7bZffsiwAOAvhrAE1Xclx9gk6IREh9g06IZFCwC5EICnYhEkHBLkQiKNiFSAQFuxCJoGAXIhEU\n7EIkwv8HBWiFlhK7ycwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "lykS2F8k-i4k",
    "outputId": "71fb1db9-98d4-49f7-a733-8842db173159"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (42000, 32, 32)\n",
      "x_test shape: (18000, 32, 32)\n",
      "42000 train samples\n",
      "18000 test samples\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aFgP7V72lw7o"
   },
   "source": [
    "# Splitting to Train and Test Set and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "vyy3E91SMYNG",
    "outputId": "60a8ef78-9244-44f3-be6d-608e323605c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (42000, 1024)\n",
      "x_test shape: (18000, 1024)\n",
      "42000 train samples\n",
      "18000 test samples\n"
     ]
    }
   ],
   "source": [
    "nsamples_train, nx_train, ny_train = x_train.shape\n",
    "nsamples_test, nx_test, ny_test = x_test.shape\n",
    "x_train = x_train.reshape((nsamples_train,nx_train*ny_train))\n",
    "x_test = x_test.reshape((nsamples_test,nx_test*ny_test))\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#Normalizing the input\n",
    "x_train /= 255.0\n",
    "x_test /= 255.0\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QHOy_QKFlowh"
   },
   "source": [
    "# Converting the labels to binary matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YawbZcx25BuT"
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O69eyuvOlkin"
   },
   "source": [
    "# Creating and evaluation KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "qPpOd8DfLX3i",
    "outputId": "482e758d-2729-41c9-9f3d-eb92be424afc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1, accuracy=45.92%\n",
      "k=3, accuracy=46.18%\n",
      "k=5, accuracy=49.02%\n",
      "k=7, accuracy=50.71%\n",
      "k=9, accuracy=51.24%\n",
      "k=11, accuracy=51.80%\n",
      "k=13, accuracy=52.11%\n",
      "k=15, accuracy=52.36%\n",
      "k=17, accuracy=52.87%\n",
      "k=19, accuracy=52.72%\n",
      "k=21, accuracy=52.91%\n",
      "k=23, accuracy=52.98%\n",
      "k=25, accuracy=52.97%\n",
      "k=27, accuracy=53.23%\n",
      "k=29, accuracy=53.16%\n"
     ]
    }
   ],
   "source": [
    "# initialize the values of k for our k-Nearest Neighbor classifier along with the\n",
    "# list of accuracies for each value of k\n",
    "\n",
    "kVals = range(1, 30, 2)\n",
    "accuracies = []\n",
    "\n",
    "# loop over various values of `k` for the k-Nearest Neighbor classifier\n",
    "\n",
    "for k in range(1, 30, 2):\n",
    "          # train the k-Nearest Neighbor classifier with the current value of `k`\n",
    "          model = KNeighborsClassifier(n_neighbors=k)\n",
    "          model.fit(x_train, y_train)\n",
    "          # evaluate the model and update the accuracies list\n",
    "          score = model.score(x_test, y_test)\n",
    "          print(\"k=%d, accuracy=%.2f%%\" % (k, score * 100))\n",
    "        \n",
    "          accuracies.append(score)          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i5A9ug4Cy9_I"
   },
   "source": [
    "# Build the classification NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "O3LVJP6Z5Thw",
    "outputId": "6d25d668-374d-4bca-8a18-afc3aca0c91d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "#def build_model(optimizer, learning_rate):\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(200, kernel_initializer='random_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(100, kernel_initializer='random_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(100, kernel_initializer='random_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(100, kernel_initializer='random_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(30, kernel_initializer='random_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(100, kernel_initializer='random_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(10, kernel_initializer='random_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hwBW02lfy-7X"
   },
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=0.01, decay = 1e-7, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IvBT0bumy7Ev"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yeGEpA0slZul"
   },
   "source": [
    "# Training and Evaluation of the NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "M8oyFHBV7V7T",
    "outputId": "4259a4c9-57b4-4f33-a0ee-e9c7079fd582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/100\n",
      "33600/33600 [==============================] - 3s 77us/sample - loss: 0.3483 - acc: 0.8973 - val_loss: 0.3252 - val_acc: 0.9000\n",
      "Epoch 2/100\n",
      "33600/33600 [==============================] - 1s 33us/sample - loss: 0.3260 - acc: 0.9000 - val_loss: 0.3254 - val_acc: 0.9000\n",
      "Epoch 3/100\n",
      "33600/33600 [==============================] - 1s 35us/sample - loss: 0.3242 - acc: 0.9000 - val_loss: 0.3254 - val_acc: 0.9000\n",
      "Epoch 4/100\n",
      "33600/33600 [==============================] - 1s 33us/sample - loss: 0.3222 - acc: 0.9000 - val_loss: 0.3250 - val_acc: 0.9000\n",
      "Epoch 5/100\n",
      "33600/33600 [==============================] - 1s 37us/sample - loss: 0.3173 - acc: 0.9000 - val_loss: 0.3223 - val_acc: 0.9000\n",
      "Epoch 6/100\n",
      "33600/33600 [==============================] - 1s 35us/sample - loss: 0.3094 - acc: 0.9001 - val_loss: 0.3117 - val_acc: 0.9000\n",
      "Epoch 7/100\n",
      "33600/33600 [==============================] - 1s 35us/sample - loss: 0.2990 - acc: 0.9010 - val_loss: 0.2947 - val_acc: 0.9007\n",
      "Epoch 8/100\n",
      "33600/33600 [==============================] - 1s 36us/sample - loss: 0.2911 - acc: 0.9015 - val_loss: 0.2826 - val_acc: 0.9021\n",
      "Epoch 9/100\n",
      "33600/33600 [==============================] - 1s 36us/sample - loss: 0.2847 - acc: 0.9021 - val_loss: 0.2724 - val_acc: 0.9034\n",
      "Epoch 10/100\n",
      "33600/33600 [==============================] - 1s 36us/sample - loss: 0.2767 - acc: 0.9030 - val_loss: 0.2586 - val_acc: 0.9062\n",
      "Epoch 11/100\n",
      "33600/33600 [==============================] - 1s 36us/sample - loss: 0.2676 - acc: 0.9042 - val_loss: 0.2454 - val_acc: 0.9090\n",
      "Epoch 12/100\n",
      "33600/33600 [==============================] - 1s 33us/sample - loss: 0.2602 - acc: 0.9055 - val_loss: 0.2355 - val_acc: 0.9117\n",
      "Epoch 13/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.2527 - acc: 0.9071 - val_loss: 0.2250 - val_acc: 0.9150\n",
      "Epoch 14/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.2465 - acc: 0.9088 - val_loss: 0.2199 - val_acc: 0.9163\n",
      "Epoch 15/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.2390 - acc: 0.9107 - val_loss: 0.2117 - val_acc: 0.9206\n",
      "Epoch 16/100\n",
      "33600/33600 [==============================] - 1s 35us/sample - loss: 0.2329 - acc: 0.9124 - val_loss: 0.2081 - val_acc: 0.9241\n",
      "Epoch 17/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.2266 - acc: 0.9145 - val_loss: 0.1957 - val_acc: 0.9273\n",
      "Epoch 18/100\n",
      "33600/33600 [==============================] - 1s 33us/sample - loss: 0.2226 - acc: 0.9158 - val_loss: 0.1927 - val_acc: 0.9295\n",
      "Epoch 19/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.2174 - acc: 0.9180 - val_loss: 0.1849 - val_acc: 0.9327\n",
      "Epoch 20/100\n",
      "33600/33600 [==============================] - 1s 33us/sample - loss: 0.2122 - acc: 0.9203 - val_loss: 0.1784 - val_acc: 0.9358\n",
      "Epoch 21/100\n",
      "33600/33600 [==============================] - 1s 36us/sample - loss: 0.2079 - acc: 0.9225 - val_loss: 0.1754 - val_acc: 0.9376\n",
      "Epoch 22/100\n",
      "33600/33600 [==============================] - 1s 33us/sample - loss: 0.2022 - acc: 0.9250 - val_loss: 0.1677 - val_acc: 0.9400\n",
      "Epoch 23/100\n",
      "33600/33600 [==============================] - 1s 35us/sample - loss: 0.1989 - acc: 0.9267 - val_loss: 0.1680 - val_acc: 0.9403\n",
      "Epoch 24/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1951 - acc: 0.9285 - val_loss: 0.1707 - val_acc: 0.9399\n",
      "Epoch 25/100\n",
      "33600/33600 [==============================] - 1s 36us/sample - loss: 0.1905 - acc: 0.9306 - val_loss: 0.1593 - val_acc: 0.9447\n",
      "Epoch 26/100\n",
      "33600/33600 [==============================] - 1s 33us/sample - loss: 0.1881 - acc: 0.9317 - val_loss: 0.1578 - val_acc: 0.9450\n",
      "Epoch 27/100\n",
      "33600/33600 [==============================] - 1s 33us/sample - loss: 0.1839 - acc: 0.9336 - val_loss: 0.1574 - val_acc: 0.9455\n",
      "Epoch 28/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1815 - acc: 0.9348 - val_loss: 0.1477 - val_acc: 0.9496\n",
      "Epoch 29/100\n",
      "33600/33600 [==============================] - 1s 35us/sample - loss: 0.1797 - acc: 0.9356 - val_loss: 0.1451 - val_acc: 0.9505\n",
      "Epoch 30/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1747 - acc: 0.9378 - val_loss: 0.1419 - val_acc: 0.9515\n",
      "Epoch 31/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1733 - acc: 0.9385 - val_loss: 0.1439 - val_acc: 0.9505\n",
      "Epoch 32/100\n",
      "33600/33600 [==============================] - 1s 36us/sample - loss: 0.1707 - acc: 0.9397 - val_loss: 0.1400 - val_acc: 0.9524\n",
      "Epoch 33/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1667 - acc: 0.9409 - val_loss: 0.1434 - val_acc: 0.9503\n",
      "Epoch 34/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1657 - acc: 0.9419 - val_loss: 0.1408 - val_acc: 0.9512\n",
      "Epoch 35/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1623 - acc: 0.9435 - val_loss: 0.1272 - val_acc: 0.9567\n",
      "Epoch 36/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1610 - acc: 0.9437 - val_loss: 0.1308 - val_acc: 0.9548\n",
      "Epoch 37/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1587 - acc: 0.9442 - val_loss: 0.1540 - val_acc: 0.9459\n",
      "Epoch 38/100\n",
      "33600/33600 [==============================] - 1s 33us/sample - loss: 0.1560 - acc: 0.9457 - val_loss: 0.1276 - val_acc: 0.9568\n",
      "Epoch 39/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1548 - acc: 0.9463 - val_loss: 0.1421 - val_acc: 0.9508\n",
      "Epoch 40/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1526 - acc: 0.9472 - val_loss: 0.1275 - val_acc: 0.9558\n",
      "Epoch 41/100\n",
      "33600/33600 [==============================] - 1s 35us/sample - loss: 0.1500 - acc: 0.9481 - val_loss: 0.1295 - val_acc: 0.9559\n",
      "Epoch 42/100\n",
      "33600/33600 [==============================] - 1s 35us/sample - loss: 0.1488 - acc: 0.9485 - val_loss: 0.1287 - val_acc: 0.9559\n",
      "Epoch 43/100\n",
      "33600/33600 [==============================] - 1s 35us/sample - loss: 0.1482 - acc: 0.9489 - val_loss: 0.1213 - val_acc: 0.9584\n",
      "Epoch 44/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1460 - acc: 0.9494 - val_loss: 0.1353 - val_acc: 0.9536\n",
      "Epoch 45/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1452 - acc: 0.9499 - val_loss: 0.1155 - val_acc: 0.9603\n",
      "Epoch 46/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1442 - acc: 0.9503 - val_loss: 0.1174 - val_acc: 0.9601\n",
      "Epoch 47/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1425 - acc: 0.9513 - val_loss: 0.1169 - val_acc: 0.9598\n",
      "Epoch 48/100\n",
      "33600/33600 [==============================] - 1s 35us/sample - loss: 0.1406 - acc: 0.9514 - val_loss: 0.1136 - val_acc: 0.9613\n",
      "Epoch 49/100\n",
      "33600/33600 [==============================] - 1s 33us/sample - loss: 0.1396 - acc: 0.9521 - val_loss: 0.1210 - val_acc: 0.9591\n",
      "Epoch 50/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1387 - acc: 0.9523 - val_loss: 0.1143 - val_acc: 0.9609\n",
      "Epoch 51/100\n",
      "33600/33600 [==============================] - 1s 35us/sample - loss: 0.1380 - acc: 0.9525 - val_loss: 0.1184 - val_acc: 0.9589\n",
      "Epoch 52/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1360 - acc: 0.9533 - val_loss: 0.1094 - val_acc: 0.9631\n",
      "Epoch 53/100\n",
      "33600/33600 [==============================] - 1s 35us/sample - loss: 0.1355 - acc: 0.9535 - val_loss: 0.1205 - val_acc: 0.9594\n",
      "Epoch 54/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1340 - acc: 0.9541 - val_loss: 0.1130 - val_acc: 0.9616\n",
      "Epoch 55/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1337 - acc: 0.9543 - val_loss: 0.1177 - val_acc: 0.9599\n",
      "Epoch 56/100\n",
      "33600/33600 [==============================] - 1s 36us/sample - loss: 0.1329 - acc: 0.9548 - val_loss: 0.1149 - val_acc: 0.9608\n",
      "Epoch 57/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1306 - acc: 0.9554 - val_loss: 0.1142 - val_acc: 0.9609\n",
      "Epoch 58/100\n",
      "33600/33600 [==============================] - 1s 33us/sample - loss: 0.1292 - acc: 0.9561 - val_loss: 0.1121 - val_acc: 0.9617\n",
      "Epoch 59/100\n",
      "33600/33600 [==============================] - 1s 33us/sample - loss: 0.1284 - acc: 0.9563 - val_loss: 0.1054 - val_acc: 0.9644\n",
      "Epoch 60/100\n",
      "33600/33600 [==============================] - 1s 36us/sample - loss: 0.1292 - acc: 0.9559 - val_loss: 0.1259 - val_acc: 0.9572\n",
      "Epoch 61/100\n",
      "33600/33600 [==============================] - 1s 33us/sample - loss: 0.1288 - acc: 0.9562 - val_loss: 0.1191 - val_acc: 0.9583\n",
      "Epoch 62/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1261 - acc: 0.9565 - val_loss: 0.1102 - val_acc: 0.9624\n",
      "Epoch 63/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1253 - acc: 0.9569 - val_loss: 0.1102 - val_acc: 0.9625\n",
      "Epoch 64/100\n",
      "33600/33600 [==============================] - 1s 33us/sample - loss: 0.1248 - acc: 0.9573 - val_loss: 0.1203 - val_acc: 0.9586\n",
      "Epoch 65/100\n",
      "33600/33600 [==============================] - 1s 35us/sample - loss: 0.1257 - acc: 0.9572 - val_loss: 0.1215 - val_acc: 0.9573\n",
      "Epoch 66/100\n",
      "33600/33600 [==============================] - 1s 33us/sample - loss: 0.1243 - acc: 0.9575 - val_loss: 0.1036 - val_acc: 0.9655\n",
      "Epoch 67/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1244 - acc: 0.9576 - val_loss: 0.1161 - val_acc: 0.9598\n",
      "Epoch 68/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1217 - acc: 0.9586 - val_loss: 0.1036 - val_acc: 0.9647\n",
      "Epoch 69/100\n",
      "33600/33600 [==============================] - 1s 33us/sample - loss: 0.1215 - acc: 0.9584 - val_loss: 0.1019 - val_acc: 0.9650\n",
      "Epoch 70/100\n",
      "33600/33600 [==============================] - 1s 35us/sample - loss: 0.1206 - acc: 0.9589 - val_loss: 0.1074 - val_acc: 0.9634\n",
      "Epoch 71/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1204 - acc: 0.9590 - val_loss: 0.1191 - val_acc: 0.9587\n",
      "Epoch 72/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1214 - acc: 0.9584 - val_loss: 0.1069 - val_acc: 0.9640\n",
      "Epoch 73/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1195 - acc: 0.9594 - val_loss: 0.1073 - val_acc: 0.9625\n",
      "Epoch 74/100\n",
      "33600/33600 [==============================] - 1s 35us/sample - loss: 0.1192 - acc: 0.9597 - val_loss: 0.1065 - val_acc: 0.9634\n",
      "Epoch 75/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1183 - acc: 0.9598 - val_loss: 0.1107 - val_acc: 0.9623\n",
      "Epoch 76/100\n",
      "33600/33600 [==============================] - 1s 33us/sample - loss: 0.1184 - acc: 0.9601 - val_loss: 0.0993 - val_acc: 0.9661\n",
      "Epoch 77/100\n",
      "33600/33600 [==============================] - 1s 35us/sample - loss: 0.1171 - acc: 0.9599 - val_loss: 0.1008 - val_acc: 0.9656\n",
      "Epoch 78/100\n",
      "33600/33600 [==============================] - 1s 35us/sample - loss: 0.1163 - acc: 0.9603 - val_loss: 0.0989 - val_acc: 0.9666\n",
      "Epoch 79/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1165 - acc: 0.9605 - val_loss: 0.0938 - val_acc: 0.9679\n",
      "Epoch 80/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1153 - acc: 0.9606 - val_loss: 0.1103 - val_acc: 0.9612\n",
      "Epoch 81/100\n",
      "33600/33600 [==============================] - 1s 35us/sample - loss: 0.1156 - acc: 0.9609 - val_loss: 0.1025 - val_acc: 0.9649\n",
      "Epoch 82/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1133 - acc: 0.9618 - val_loss: 0.1035 - val_acc: 0.9646\n",
      "Epoch 83/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1134 - acc: 0.9616 - val_loss: 0.1027 - val_acc: 0.9647\n",
      "Epoch 84/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1127 - acc: 0.9617 - val_loss: 0.0989 - val_acc: 0.9657\n",
      "Epoch 85/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1136 - acc: 0.9620 - val_loss: 0.1181 - val_acc: 0.9594\n",
      "Epoch 86/100\n",
      "33600/33600 [==============================] - 1s 33us/sample - loss: 0.1115 - acc: 0.9618 - val_loss: 0.1042 - val_acc: 0.9644\n",
      "Epoch 87/100\n",
      "33600/33600 [==============================] - 1s 33us/sample - loss: 0.1118 - acc: 0.9625 - val_loss: 0.0963 - val_acc: 0.9670\n",
      "Epoch 88/100\n",
      "33600/33600 [==============================] - 1s 35us/sample - loss: 0.1108 - acc: 0.9625 - val_loss: 0.0976 - val_acc: 0.9667\n",
      "Epoch 89/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1105 - acc: 0.9624 - val_loss: 0.1167 - val_acc: 0.9599\n",
      "Epoch 90/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1104 - acc: 0.9628 - val_loss: 0.1027 - val_acc: 0.9642\n",
      "Epoch 91/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1103 - acc: 0.9629 - val_loss: 0.0988 - val_acc: 0.9661\n",
      "Epoch 92/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1089 - acc: 0.9630 - val_loss: 0.1036 - val_acc: 0.9645\n",
      "Epoch 93/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1092 - acc: 0.9629 - val_loss: 0.1014 - val_acc: 0.9650\n",
      "Epoch 94/100\n",
      "33600/33600 [==============================] - 1s 33us/sample - loss: 0.1089 - acc: 0.9632 - val_loss: 0.0969 - val_acc: 0.9674\n",
      "Epoch 95/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1072 - acc: 0.9636 - val_loss: 0.1025 - val_acc: 0.9646\n",
      "Epoch 96/100\n",
      "33600/33600 [==============================] - 1s 35us/sample - loss: 0.1073 - acc: 0.9635 - val_loss: 0.0946 - val_acc: 0.9679\n",
      "Epoch 97/100\n",
      "33600/33600 [==============================] - 1s 33us/sample - loss: 0.1063 - acc: 0.9639 - val_loss: 0.1115 - val_acc: 0.9621\n",
      "Epoch 98/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1087 - acc: 0.9632 - val_loss: 0.0943 - val_acc: 0.9684\n",
      "Epoch 99/100\n",
      "33600/33600 [==============================] - 1s 33us/sample - loss: 0.1070 - acc: 0.9637 - val_loss: 0.0944 - val_acc: 0.9674\n",
      "Epoch 100/100\n",
      "33600/33600 [==============================] - 1s 34us/sample - loss: 0.1058 - acc: 0.9643 - val_loss: 0.0950 - val_acc: 0.9674\n",
      "Calculating performance on test set...\n",
      "Test loss: 0.0960\n",
      "Test accuracy: 0.9668\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs=100,batch_size=500, validation_split=0.2)\n",
    "\n",
    "# calculate and store test set performance on the model with best validation error\n",
    "print(\"Calculating performance on test set...\")\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss: {:.4f}'.format(score[0]))\n",
    "print('Test accuracy: {:.4f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4mraPVLQlN37"
   },
   "source": [
    "## Review model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "lmUt-ttdxffJ",
    "outputId": "d46de6cd-1407-4eec-9980-e9ac27aabaab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  205000    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo multiple                  800       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  20100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch multiple                  400       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch multiple                  400       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch multiple                  400       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  3030      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch multiple                  120       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  3100      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch multiple                  400       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  1010      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch multiple                  40        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    multiple                  0         \n",
      "=================================================================\n",
      "Total params: 255,000\n",
      "Trainable params: 253,720\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eMsXv0AVlSkX"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "<font color='blue'>As per above, we could see NN works much better than KNN in terms of both Accuracy <b>(KNN-53% / NN-96%)</b> and Performance.\n",
    "<br>Evaluation using KNN model, takes pretty long time.\n",
    "<br>NN performance is quite good ( pretty fast)\n",
    "<br>This proved our theory that KNN takes longer time if the dataset is bigger one, but NN works pretty good when dataset is bigger.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r50GyhMplVpm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NN Project 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
